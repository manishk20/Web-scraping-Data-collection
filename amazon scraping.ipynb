{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession\n",
    "s = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the categories to extract\n",
    "\n",
    "men_clothing = ['https://www.amazon.in/s?k=men+clothing&ref=nb_sb_noss_2','men clothing']\n",
    "women_clothing = ['https://www.amazon.in/s?k=women+clothing&ref=nb_sb_noss_2','women clothing']\n",
    "footwear = ['https://www.amazon.in/s?k=footwear&ref=nb_sb_noss_2','footwear']\n",
    "watches = ['https://www.amazon.in/s?k=watches&ref=nb_sb_noss_2','watches']\n",
    "wallets = ['https://www.amazon.in/s?k=wallets&ref=nb_sb_noss_1','wallets']\n",
    "bags = ['https://www.amazon.in/s?k=bags&ref=nb_sb_noss_2','bags']\n",
    "jewellery = ['https://www.amazon.in/s?k=jewellery&ref=nb_sb_noss_2','jewellery']\n",
    "belts = ['https://www.amazon.in/s?k=belts&ref=nb_sb_noss_1','belts']\n",
    "ties = ['https://www.amazon.in/s?k=ties&ref=nb_sb_noss_1','ties']\n",
    "cufflinks = ['https://www.amazon.in/s?k=cufflinks&ref=nb_sb_noss_1','cufflinks']\n",
    "pocket_squares = ['https://www.amazon.in/s?k=pocket+squares&ref=nb_sb_noss_2','pocket_squares']\n",
    "caps_and_hats = ['https://www.amazon.in/s?k=caps+and+hats&ref=nb_sb_noss_1','caps and hats']\n",
    "mufflers_scarves_gloves = ['https://www.amazon.in/s?k=mufflers%2C+scarves+and+gloves&ref=nb_sb_noss','mufflers, scraves and gloves']\n",
    "phones_cases = ['https://www.amazon.in/s?k=phone+cases&ref=nb_sb_noss_2','phones cases']\n",
    "rings_and_wristwear = ['https://www.amazon.in/s?k=rings+and+wristwear&ref=nb_sb_noss','rings and wristwear']\n",
    "socks = ['https://www.amazon.in/s?k=socks&ref=nb_sb_noss','socks']\n",
    "bracelets = ['https://www.amazon.in/s?k=bracelets&ref=nb_sb_noss_2','bracelets']\n",
    "chains = ['https://www.amazon.in/s?k=chains&ref=nb_sb_noss_1','chains']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of categories\n",
    "\n",
    "category_list = [men_clothing,women_clothing,footwear,watches,wallets,bags,jewellery,belts,ties,cufflinks,pocket_squares,caps_and_hats,mufflers_scarves_gloves,phones_cases,rings_and_wristwear,socks,bracelets,chains]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for extraction and processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract data from the webpage\n",
    "\n",
    "def get_data(url):\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    return soup\n",
    "\n",
    "# funtion to extract link for the next page in amazon.in of the category of the product\n",
    "\n",
    "def get_nextpage(soup1):\n",
    "    page1 = soup1.find('ul',{'class':'a-pagination'})\n",
    "    try:\n",
    "        if not page1.find('li',{'class':'a-disabled a-last'}):\n",
    "            url = 'https://www.amazon.in/'+str(page1.find('li',{'class':'a-last'}).find('a')['href'])\n",
    "            return url\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return 'load again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get sizes of the product \n",
    "\n",
    "def get_sizes(p_soup):\n",
    "    sizes=[]\n",
    "    page2 = p_soup\n",
    "    size = page2.find('select',{'name':'dropdown_selected_size_name'})\n",
    "    if size:\n",
    "        s_list = size.find_all('option')\n",
    "        for i in s_list:\n",
    "            sizes.append(i.get_text(strip=True))\n",
    "    else:\n",
    "        sizes=[None]\n",
    "    sizes.pop(0)\n",
    "    if len(sizes)==0:\n",
    "        sizes = None\n",
    "    return sizes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get secondary images list of the product \n",
    "\n",
    "def get_secondary_image_links(p_soup):\n",
    "    page3 = p_soup\n",
    "    im_list = []\n",
    "    images = page3.find('ul',{'class':'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-extra-large'})\n",
    "    if images and len(images.find_all('li',{'class':'a-spacing-small item'})):\n",
    "        for i in images.find_all('li',{'class':'a-spacing-small item'}):\n",
    "            im_list.append(i.find('img')['src'])\n",
    "    else:\n",
    "        im_list = [None]\n",
    "    im_list.pop(0)\n",
    "    if len(im_list)==0:\n",
    "        im_list=None\n",
    "    return im_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get description of the product \n",
    "\n",
    "def get_description(p_soup):\n",
    "    page = p_soup\n",
    "    desc = page.find('div',{'id':'productDescription'})\n",
    "    description = []\n",
    "    if desc:\n",
    "        for i in desc.stripped_strings:\n",
    "            description.append('->  '+i+'\\n\\n')\n",
    "        return ''.join(description)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the needed information about all the products from amazon.in\n",
    "\n",
    "def get_category_info(search_url):\n",
    "    current_page_link = search_url[0]\n",
    "    current_page = get_data(current_page_link)\n",
    "    info = []\n",
    "    #get all prod data \n",
    "    print(\"\\n\\n\\nEXTRACTING DIFFERENT PRODUCT CATEGORY\",end='')\n",
    "    x=1\n",
    "    while True:\n",
    "        next_page=None\n",
    "        #loading the page properly\n",
    "        while get_nextpage(current_page)=='load again':\n",
    "            current_page = get_data(current_page_link)\n",
    "        if get_nextpage(current_page):\n",
    "            next_page = get_data(get_nextpage(current_page))\n",
    "        print(\".....extracting \"+str(x)+\"th page\",end='')\n",
    "        p = current_page.find_all('div',{'data-component-type':'s-search-result'})\n",
    "        for l in p:\n",
    "            \n",
    "            # extracting gender using name of the product\n",
    "            \n",
    "            gender = None\n",
    "            product_name = l.find('span',{'class':'a-size-base-plus a-color-base a-text-normal'})\n",
    "            if product_name:\n",
    "                product_name = str(product_name.get_text())\n",
    "                if ' men' in search_url[1].lower()+product_name.lower() and 'women' in search_url[1].lower()+product_name.lower():\n",
    "                    gender = 'any'\n",
    "                elif 'women' in search_url[1].lower()+product_name.lower():\n",
    "                    gender = 'female'\n",
    "                elif ' men' in search_url[1].lower()+product_name.lower():\n",
    "                    gender = 'male'\n",
    "                else:\n",
    "                    gender = 'any'\n",
    "\n",
    "            else:\n",
    "                product_name = None\n",
    "                \n",
    "                            \n",
    "            # extracting brand name \n",
    "            \n",
    "            brand = l.find('span',{'class':'a-size-base-plus a-color-base'})\n",
    "            if brand:\n",
    "                brand = str(brand.get_text())\n",
    "            else:\n",
    "                brand = None\n",
    "                \n",
    "            # extracting price \n",
    "            \n",
    "            price = l.find('span',{'class':'a-price-whole'})\n",
    "            if price:\n",
    "                if ',' in price.get_text():\n",
    "                    price = float(''.join(price.get_text().split(',')))\n",
    "                else:\n",
    "                    price = float(price.get_text())\n",
    "            else:\n",
    "                price = 500\n",
    "           \n",
    "                \n",
    "            # extracting MRP \n",
    "            MRP = l.find('span',{'class':'a-price a-text-price'})\n",
    "            if MRP:\n",
    "                MRP=MRP.find('span',{'aria-hidden':'true'}).get_text()[1:] \n",
    "                if ',' in MRP:\n",
    "                    MRP=float(''.join(MRP.split(',')))\n",
    "                else:\n",
    "                    MRP = float(MRP)\n",
    "            else:\n",
    "                MRP = 1000\n",
    "                \n",
    "            product_link = 'https://www.amazon.in'+str(l.find('a',{'class':'a-link-normal s-no-outline'})['href'])\n",
    "            product_soup = get_data(product_link)\n",
    "            \n",
    "            # collecting all data\n",
    "            d = {\n",
    "                'website':'https://www.amazon.in',\n",
    "                'product_name':product_name,\n",
    "                'product_brand':brand,\n",
    "                'product_category':search_url[1],\n",
    "                'sizes_available': str(get_sizes(product_soup))[1:-1],\n",
    "                'price':price,\n",
    "                'MRP':MRP,\n",
    "                'gender':gender,\n",
    "                'description': get_description(product_soup),\n",
    "                'primary_image_link':l.find('img',{'class':'s-image'})['src'],\n",
    "                'secondary_image_links':str(get_secondary_image_links(product_soup))[1:-1],\n",
    "                'product_link': product_link\n",
    "            }\n",
    "            info.append(d)\n",
    "        x+=1\n",
    "        if next_page:\n",
    "            current_page_link = get_nextpage(current_page)\n",
    "            current_page = next_page\n",
    "        else:\n",
    "            break\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting whole data and Building the Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page.....extracting 8th page.....extracting 9th page.....extracting 10th page.....extracting 11th page.....extracting 12th page.....extracting 13th page.....extracting 14th page.....extracting 15th page.....extracting 16th page.....extracting 17th page.....extracting 18th page.....extracting 19th page.....extracting 20th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page.....extracting 8th page.....extracting 9th page.....extracting 10th page.....extracting 11th page.....extracting 12th page.....extracting 13th page.....extracting 14th page.....extracting 15th page.....extracting 16th page.....extracting 17th page.....extracting 18th page.....extracting 19th page.....extracting 20th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page.....extracting 7th page\n",
      "extracting different product category.....extracting 1th page.....extracting 2th page.....extracting 3th page.....extracting 4th page.....extracting 5th page.....extracting 6th page"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "for item in category_list:\n",
    "    j=get_category_info(item)\n",
    "    df2 = pd.DataFrame(j)\n",
    "    df1 = df1.append(df2, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing dataframe in sqLite3 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "engine = sqlite3.connect('AllData.db')\n",
    "c = engine.cursor()\n",
    "\n",
    "try:\n",
    "    c.execute('CREATE TABLE PRODUCTS (website,product_name,product_brand,product_category,sizes_available,price,MRP,gender,description,primary_image_link,secondary_image_links,product_link)')\n",
    "    engine.commit()\n",
    "    \n",
    "except Exception as e:\n",
    "    if str(e)=='table PRODUCTS already exists' and str(e.__class__) == \"<class 'sqlite3.OperationalError'>\":\n",
    "        c.execute('DROP TABLE PRODUCTS')\n",
    "        engine.commit()\n",
    "\n",
    "        df1.to_sql('PRODUCTS',engine, if_exists='replace', index = False)\n",
    "    else:\n",
    "        print(e.__class__,e)\n",
    "else:\n",
    "    df1.to_sql('PRODUCTS',engine, if_exists='replace', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the database and showing data stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "\n",
    "with closing(sqlite3.connect(\"AllData.db\")) as engine:\n",
    "    with closing(engine.cursor()) as c:\n",
    "        c.execute('''SELECT * FROM PRODUCTS''')\n",
    "        for row in c.fetchall():\n",
    "            print(row)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
